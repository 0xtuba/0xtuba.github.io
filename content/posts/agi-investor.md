---
title: "ASI Superforecaster"
date: 2025-02-12T12:00:00+00:00
draft: false
---

## Introduction 
An **AGI superforecaster** refers to a hypothetical artificial general intelligence that can predict real-world events with superhuman accuracy across domains. *Artificial General Intelligence (AGI)* is defined as an AI system that matches or exceeds human cognitive capabilities across a wide range of tasks ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=Artificial%20general%20intelligence%20,the%20definitions%20of%20%20162)). In this context, an AGI superforecaster would possess broad knowledge and reasoning skills enabling it to forecast events in politics, economics, science, and other fields better than any human. The term *“superforecaster”* originally describes humans who consistently make more accurate predictions than others (even outperforming domain experts) ([Superforecaster - Wikipedia](https://en.wikipedia.org/wiki/Superforecaster#:~:text=Forecasters%20whose%20results%20are%20more,accurate%20than%20average)). Combining these concepts, an AGI superforecaster would be an AI with general intelligence that achieves the level of a top-tier human forecaster – or beyond – in predictive accuracy.

The significance of developing such an AI is profound. Accurate forecasting is crucial for informed decision-making: for example, economic forecasts guide investments and hiring, and early pandemic predictions prompted lockdowns to save lives ([[2409.19839] ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities](https://ar5iv.org/abs/2409.19839#:~:text=Forecasting%20the%20future%20is%20a,6%2C%20Yan%20et%C2%A0al)). A superhuman forecasting system could drastically improve our ability to anticipate and respond to global events, potentially leading to better policies, early warnings for crises, and more efficient resource allocation. In short, an AGI superforecaster promises to *“bring the future into focus”* and enhance human foresight, which could be invaluable for government planning, businesses, and society at large.

## Current State of the Art 
**AI Forecasting Today:** Modern AI and machine learning techniques are already used for forecasting in specific domains. For instance, machine learning models predict **financial** markets and sales trends, **healthcare** outcomes (like patient admission rates), and even **natural disasters** (flood or hurricane risks) with increasing accuracy ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=Nowadays%C2%A0predictions%20using%20ML%20and%20AI%C2%A0are,that%20need%20to%20be%20overcome)). These systems typically rely on pattern recognition in large historical datasets. In narrow, data-rich contexts, AI can rival or surpass human analysts – e.g. e-commerce companies use AI to accurately forecast product demand by analyzing sales data alongside factors like seasonality ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=%2A%20E,by%20considering%20factors)). However, these successes are mostly in well-structured problems. Forecasting complex geopolitical or social events remains far more challenging, as it involves dynamic, open-ended variables that are hard to capture in data.

**Human vs Machine Forecasting:** Notably, the best human forecasters still hold an edge in many real-world prediction tasks. In a U.S. government-sponsored tournament, Good Judgment Project’s elite human “Superforecasters” went head-to-head with cutting-edge hybrid systems (which combined algorithms with inputs from crowds of forecasters). The result: the Superforecasters were about **20% more accurate** than the next-best hybrid human-machine team ([Supers vs hybrid systems - Good Judgment](https://goodjudgment.com/resources/the-superforecasters-track-record/supers-vs-hybrid-systems/#:~:text=In%20a%20US,later%2C%20the%20results%20were%20clear)). This suggests that current AI-driven approaches, even when aided by human judgment, have yet to exceed the calibrated reasoning and adaptive thinking of top humans in complex forecasting scenarios. Similarly, recent benchmarks comparing AI to human forecasters found that the **best large language model (LLM)** available still lagged expert human forecasters in accuracy by roughly **19%** ([The Science Of Superforecasting – Good Judgment](https://goodjudgment.com/about/the-science-of-superforecasting/#:~:text=%282024%29%20arxiv)). In other words, while AI models have achieved *superhuman performance on many structured tasks*, they perform less well on open-ended future prediction questions, still underperforming dedicated human forecasters on average.

**Limitations of Current Systems:** Today’s AI forecasting systems face several limitations. One major issue is that they tend to be specialized—**narrow AI** excels at specific tasks but lacks broad contextual understanding. Models require vast quantities of high-quality data, and if data is **inaccurate or biased**, the predictions will reflect those flaws ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Data%20Quality%20and%20Quantity%3A%20In,a%20limitation%20for%20several%20businesses)) ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=6,makes%20will%20also%20be%20biased)). Moreover, statistical or machine learning models struggle with **novel or rare events**. They can extrapolate from historical patterns, but **“black swan”** events or sudden geopolitical shifts often fall outside what the model has seen before ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Volatility%20and%20Uncertainty%3A%20Financial%20markets,the%20pattern%20of%20historical%20data)). For example, an algorithm might be blindsided by an unprecedented crisis or an unexpected political decision because it has no precedent in the training data. Another limitation is that many AI models operate as a *“black box”*, making it hard to interpret *why* a prediction was made ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Bias%20and%20Interpretability%3A%20Biases%20in,the%20confidence%20in%20the%20forecast)). This opaqueness, combined with the possibility of errors, means human oversight is still needed. Indeed, experts note that human judgment and common-sense reasoning remain **“invaluable”** in cases where AI lacks context or nuance ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Human%20Expertise%20and%20Judgment%3A%20While,a%20struggle%20for%20AI%20models)). All these factors illustrate that, as of now, no AI system possesses the fully general, adaptive forecasting prowess that an AGI superforecaster would require.

## Requirements for AGI Superforecasting 
To achieve an AGI superforecaster, several key components and capabilities would be necessary:

- **Extensive Data Processing:** The system must ingest and analyze *massive, diverse data* from many sources (text, numbers, real-time sensor data, etc.). High-quality and comprehensive data is the fuel for accurate predictions – without enough reliable data, forecasts can be *“flawed”* or misleading ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Data%20Quality%20and%20Quantity%3A%20In,a%20limitation%20for%20several%20businesses)). An AGI forecaster would need robust data pipelines and the ability to continuously gather up-to-date information across domains, far beyond what current models handle.

- **Advanced Reasoning Ability:** Beyond pattern recognition, the AI needs human-level (or better) reasoning and general knowledge. It should understand causal relationships and be able to generalize from past events to new situations. The inability of today’s AI to **“reason or generalize beyond learned patterns”** is a fundamental barrier ([Where would reasoning AI leave human intelligence?](https://www.weforum.org/stories/2025/01/in-a-world-of-reasoning-ai-where-does-that-leave-human-intelligence/#:~:text=Where%20would%20reasoning%20AI%20leave,)). An AGI superforecaster would require strong logical reasoning, causal inference skills, and possibly even *common sense* understanding to evaluate how multiple factors interplay in an unfolding event. It would, for example, have to connect political events with economic trends or scientific developments in making a prediction – a very broad, contextual form of reasoning.

- **Uncertainty Modeling & Probabilistic Thinking:** Superforecasting is not just about making a single guess – it’s about estimating probabilities and confidence levels. Top human forecasters *“don’t think in terms of yes or no but in terms of probability”*, carefully calibrating how likely an outcome is ([The Science Of Superforecasting – Good Judgment](https://goodjudgment.com/about/the-science-of-superforecasting/#:~:text=,parts%20and%20don%E2%80%99t%20think%20holistically)). Likewise, an AGI forecaster must quantify uncertainty, providing calibrated probability estimates rather than absolute answers. It should be able to model its own uncertainty and update confidence as new data arrives. This requires sophisticated statistical modeling and self-monitoring, so that the AI remains *appropriately humble* about what it does and doesn’t know.

- **Adaptability and Continuous Learning:** Real-world situations evolve, and new information can rapidly change the odds of future outcomes. An AGI superforecaster needs to be highly **adaptive**, updating its predictions in light of fresh data or events. Human superforecasters excel at continuously revising their forecasts – research shows those who frequently *“update their forecasts by little tiny steps”* as new evidence comes in are more accurate than those who make one-off predictions ([How Good Judgment Project Uses Superforecasting | Built In](https://builtin.com/data-science/superforecasters-good-judgement#:~:text=,%E2%80%9D)). Similarly, the AI should incorporate a dynamic learning mechanism (beyond static training) to refine its models on the fly. This could involve online learning, Bayesian updating, or iterative feedback loops that allow it to adjust its reasoning over time. Adaptability also means the system should work across different domains, transferring knowledge from one area to another when relevant (a hallmark of general intelligence).

In addition to the above, other requirements would include robustness to noise (distinguishing signal from noise in data), transparency in reasoning (to build trust in its predictions), and alignment with human values/intent (ensuring it aims to make helpful and truthful forecasts). All these components together outline the formidable task of building an AGI capable of superforecasting.

## Feasibility and Unsolved Challenges 
Building an AGI superforecaster is a **daunting challenge**, and experts debate how soon it might be achieved, if at all. Many researchers are skeptical that such a system is imminent. Surveys of AI experts generally place median estimates for achieving AGI on the order of *decades* away – for example, one survey indicated a 50% chance of AI being able to fully automate complex human jobs (a proxy for AGI-level capability) only by the **2060s or later**, not by the 2020s ([
	AI Timelines and National Security: The Obstacles to AGI by 2027 | Lawfare
](https://www.lawfaremedia.org/article/ai-timelines-and-national-security--the-obstacles-to-agi-by-2027#:~:text=Even%20delaying%20the%20date%20of,could%20skew%20responses%20toward%20rapid)). While recent breakthroughs in AI (like large language models) are impressive, they still fall short of the kind of general, reliable forecasting ability that defines superforecasting. In short, achieving an AGI superforecaster in the **near future** appears unlikely; it remains a long-term goal.

Several unsolved challenges stand in the way:

- **Reasoning and Generalization:** Current AI models lack true understanding of cause and effect. They often can’t explain *why* something might happen outside of patterns they’ve seen. Endowing an AI with broad reasoning—enough to make creative predictions about never-before-seen scenarios—is an open research problem. The *“inability to generalize beyond training”* limits today’s models ([Where would reasoning AI leave human intelligence?](https://www.weforum.org/stories/2025/01/in-a-world-of-reasoning-ai-where-does-that-leave-human-intelligence/#:~:text=Where%20would%20reasoning%20AI%20leave,)), and overcoming this will likely require new AI architectures or paradigms beyond today’s deep learning.

- **Data Limitations:** For many real-world questions, we simply don’t have large, rich datasets to train on. Unique or rare events (e.g. global pandemics, once-in-a-generation political upheavals) provide little precedent. **“In order to make accurate predictions, ML and AI models need large amounts of data”**, but for one-off future events, by definition, data is scarce ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=1,events%20depend%20on%20human%20behavior)). An AGI forecaster would need innovative ways to deal with data sparsity – for instance, by synthesizing simulated data, transferring knowledge from analogous situations, or incorporating expert input. It also must handle the fact that human behavior can change in unpredictable ways that past data won’t capture ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=others,are%20many%20things%20we%20don%E2%80%99t)).

- **Unpredictability of the World:** Some aspects of the future may be fundamentally hard to predict. Even a perfect intelligence would face **irreducible uncertainty** in chaotic systems or free-will-driven domains. Weather forecasts, for example, have physical limits of predictability. Likewise, social and geopolitical events can be influenced by random or secret factors. An AGI might never reach 100% accuracy, but the goal is to be **superhuman** in its calibration and foresight. This means it must account for unexpected factors as best as possible and express when a situation is inherently uncertain. It’s an unsolved challenge to determine how an AI could recognize and communicate when a forecast is highly speculative.

- **Biases and Data Quality:** AI systems inherit biases present in their training data. If data reflects skewed or partial perspectives, the AI’s predictions will also be biased ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=6,makes%20will%20also%20be%20biased)). In high-stakes forecasting (e.g., predicting social outcomes or conflicts), biased forecasts could be dangerous or unethical. Thus, creating an AGI superforecaster entails finding ways to correct for bias and ensure balanced reasoning. This might involve feeding the AI diverse viewpoints or implementing algorithmic techniques to adjust for known biases. Ensuring **data quality** (accurate, up-to-date information) is equally critical; an AI’s forecasts are only as good as the data it bases them on.

- **Trust and Verification:** Another hurdle is the **reliability** of the AI’s reasoning. Present-day AI models can exhibit errors like hallucinations – e.g. an LLM might *confidently fabricate* information or logical steps that are incorrect. Indeed, current frontier models are noted to have issues such as *“forgetting, high computational cost, and hallucinations”*, making them unreliable as autonomous agents without human oversight ([](https://arxiv.org/pdf/2402.07862#:~:text=agency%E2%80%94the%20ability%20to%20take%20actions,This)). For an AGI forecaster, we would need breakthroughs to drastically reduce such errors. The system’s predictions might also need to be *explainable* so that human analysts can verify and understand the reasoning, especially if decisions will be made based on those forecasts. Developing AI that can **justify its predictions** or highlight evidence (instead of just giving an answer) is an active area of research.

Given these challenges, the consensus is that achieving a true AGI superforecaster is **hard to achieve in the near term**. It may require fundamentally new AI techniques and gradual progress in areas like common-sense reasoning, continuous learning, and human-AI collaboration. It’s also possible that we might first see *hybrid* approaches – AI assisting human forecasters to improve accuracy – before we ever see an entirely autonomous superforecaster. In summary, while the goal is conceptually achievable, there are significant scientific and practical hurdles to overcome, and optimism should be tempered with realism about the timeline and effort required.

## Implications for Society 
If an AGI superforecaster were developed, it would carry sweeping implications for society, with both promising benefits and serious risks. The presence of a system that can anticipate events with superhuman accuracy could transform how decisions are made at every level. Below we explore potential positive impacts and the associated concerns.

### Potential Benefits 
- **Improved Decision-Making and Policy:** A superhuman forecasting system could serve as an impartial advisor to leaders, providing early warnings and probability estimates for various scenarios. By offering **neutral, data-driven forecasts**, it could help policymakers plan more effectively and avoid wishful thinking. For example, such an AI could quantify the risk of a emerging crisis (like a pandemic or financial meltdown) well in advance, allowing governments and organizations to take preventative action. Integrated into decision processes, *forecasting AIs could act as a tempering force*, countering bias and polarization with objective predictions ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=informed%20discussions%2C%20and%20maintain%20consensus,those%20pushing%20extreme%2C%20polarized%20positions)). The result would be more informed strategies in governance, foreign policy, and economic planning, potentially leading to greater stability and public benefit.

- **Economic and Societal Stability:** Better forecasts of market trends, resource scarcities, or technological disruptions would enable businesses and economies to adjust proactively. If an AGI predicts an economic recession or a supply chain disruption, steps could be taken early to mitigate impact, smoothing out volatility. In theory, super-accurate forecasting could reduce uncertainty in business investment and government budgeting, contributing to steadier growth. Moreover, accurate scientific and environmental forecasts (for instance, predicting the spread of a new disease or the effects of climate interventions) would help society prepare and allocate resources efficiently. Historical examples show the value of foresight – recall that solid early forecasts in 2020 guided public health responses ([[2409.19839] ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities](https://ar5iv.org/abs/2409.19839#:~:text=Forecasting%20the%20future%20is%20a,6%2C%20Yan%20et%C2%A0al)). An AGI superforecaster would amplify this value across all domains.

- **Faster and Scalable Analysis:** Unlike human experts or crowds, an AI system could generate predictions **rapidly and at scale**. For instance, a recent prototype forecasting AI was able to match the accuracy of human prediction crowds (around 87% accuracy on a set of events) while operating **much faster** and more cheaply ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=87.0,markets%2C%20and%20they%E2%80%99re%20similarly%20accurate)). This suggests a mature AGI forecaster could provide real-time updates on thousands of questions simultaneously – something impossible for human forecasters to do. Such speed and scalability means even localized or niche issues could get predictive attention. This widespread coverage could be beneficial for early detection of risks (e.g., spotting the early signs of a regional conflict or a disease outbreak before it escalates).

- **Enhanced Public Discourse and Knowledge:** If the forecasts of a superintelligent predictor were made available (for example, via public dashboards or integration into news), it could improve how the public understands and discusses future events. Reliable probabilities attached to news stories might dampen rumor and speculation. As the Center for AI Safety notes, *“superhuman forecasting AIs could enable improved public discourse in an increasingly complex world”*, acting as a factual counterweight to extreme or unfounded claims ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=informed%20discussions%2C%20and%20maintain%20consensus,those%20pushing%20extreme%2C%20polarized%20positions)). Essentially, it could play a role similar to an ever-present expert fact-checker and trend analyzer. Additionally, by highlighting which outcomes are likely, it might encourage long-term thinking among policymakers and citizens, fostering a more foresight-oriented culture.

### Potential Risks 
- **Misuse and Power Imbalances:** With great predictive power comes the risk of great misuse. An AGI superforecaster could be exploited by **malicious actors**, such as authoritarian governments, rogue states, or criminals. Such an AI might enable those actors to anticipate the actions of others and stay one step ahead in conflicts, negotiations, or even law enforcement countermeasures. A report from Cambridge warns that advanced AI capabilities can be used by rogue states or terrorists for malicious purposes ([Global AI experts sound the alarm in unique report](https://www.cam.ac.uk/stories/malicious-ai-report#:~:text=Global%20AI%20experts%20sound%20the,rogue%20states%2C%20criminals%2C%20and%20terrorists)). In the context of forecasting, a government with exclusive access to a superforecaster AI might manipulate economies or societies to its advantage, or a financial operator could unfairly game the markets, knowing future events in advance. This uneven distribution of predictive power could widen global inequalities and pose security threats. It raises questions about who should control or have access to such a technology and whether regulations would be needed to prevent its misuse.

- **Overreliance and Automation Bias:** If society comes to deeply trust an AI’s predictions, there’s a danger of *overreliance*. Policymakers and the public might accept the AI’s forecasts uncritically, leading to **automation bias** – the tendency to favor suggestions from an automated system even when human judgment is needed ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=through%20an%20engineered%20prompt,not%20been%20tested%20on%20its)). This could become problematic if the AI is wrong or if the situation changes in ways the AI didn’t anticipate. Blindly following an AI prediction might sometimes lead to worse outcomes (for example, prematurely withdrawing from a region because an AI forecasted instability, which then actually triggers instability). Moreover, heavy reliance on AI forecasts could erode human analytical skills and intuition over time. If people defer all judgment to the machine, we may lose the “wisdom” part of decision-making that comes from human experience and ethical consideration. Ensuring that human decision-makers remain “in the loop” and critically evaluate AI input would be an important governance challenge.

- **Self-Fulfilling Prophecies and Market Disruption:** The very act of forecasting can influence outcomes. If a superforecaster AI predicts a particular election result or market crash with high confidence and that prediction is made public, it could sway people’s behavior in response. This phenomenon could lead to **self-fulfilling prophecies** (the prediction causes itself to come true) or, conversely, attempts to counteract the prediction (making it false). For instance, if an AI predicts a bank will fail, customers might rush to withdraw funds, *causing* the failure. Additionally, there is the concern that focusing on the most likely outcome might cause decision-makers to **neglect low-probability but high-impact risks** ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=AIs%20are%20also%20known%20to,a)). An AI might assess something as only, say, 5% likely and everyone might ignore it – but in reality that 5% event (a “tail risk”) could be catastrophic if it does occur. Balancing the AI’s influence to avoid overreactions or complacency is a delicate issue. In markets, especially, ultra-accurate forecasts could lead to instability: if everyone trades on the AI’s prediction, markets could move in extreme ways or cease to function efficiently.

- **Ethical and Governance Challenges:** The deployment of a superforecasting AI raises ethics and governance questions. One concern is **accountability** – if policies are decided based on an AI’s forecast and those decisions have harmful effects, who is responsible? Attributing accountability between the AI creators, operators, and the officials who used the advice could be complicated. There are also privacy implications: an AI striving to be all-seeing might draw on surveillance data or intrusive data collection to improve its accuracy, which could conflict with civil liberties. Another ethical issue is the potential impact on democratic processes. Would elected leaders feel pressured to always heed the AI’s predictions? If so, unelected algorithms might indirectly steer society. This kind of power shift requires careful oversight. Lastly, there is the matter of **bias and fairness** – if the AI’s predictions systematically favor certain groups or outcomes (due to biased training data), it could reinforce inequalities or prejudices in decision-making. Ensuring transparency and fairness in how the AI generates forecasts would be critical to its acceptable use in society.

In summary, an AGI superforecaster could be a double-edged sword. Its **benefits** promise a smarter, more proactive society that navigates the future with greater confidence and fewer surprises. However, the **risks** underline that such power must be managed with stringent safeguards, oversight, and ethical considerations to prevent misuse and unintended consequences. Society would need new policies and perhaps international agreements to govern the use of superhuman predictive systems, much as we consider regulation for other powerful technologies.

## Conclusion 
The vision of an AGI superforecaster – an all-purpose, superhumanly accurate prediction machine – remains a compelling but challenging prospect. In this report, we defined what an AGI superforecaster means: essentially an AI with general intelligence that can consistently out-predict the best human experts across a range of domains. We saw that **current AI forecasting capabilities**, while advancing, are still narrow and fall short of human expert-level performance in complex, open-ended predictions. Building a true AGI superforecaster would require breakthroughs in data processing, reasoning, uncertainty handling, and adaptivity, among other areas. At present, significant **hurdles** – from technical limitations in AI to fundamental issues of unpredictability and bias – make this an ambitious long-term goal rather than an immediate reality. Many experts anticipate that achieving such a system will likely take decades, if it is achievable at all, given the unresolved scientific challenges.

Despite the uncertainty in timeline, the **implications** of success are enormous. If realized, an AGI superforecaster could enhance decision-making in government, business, and daily life, potentially averting crises and optimizing outcomes with its foresight. Yet it could also introduce new risks related to misuse, overreliance, and ethical governance of AI power. The prospect calls for a cautious approach: research and development should go hand in hand with ethical guidelines and oversight frameworks to ensure that any progress in superforecasting AI truly benefits society. As AI systems grow more powerful, it will be crucial that their **forecasting capabilities help make us more prudent, not just more prescient** – increasing our collective wisdom about the future, not just our knowledge ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=Carl%20Sagan%20noted%2C%20%E2%80%9CIf%20we,prudent%20and%20increase%20our%20foresight)). In the end, the development of an AGI superforecaster is not just a technical endeavor, but a societal one, requiring collaboration between technologists, policymakers, and ethicists. With careful advancement, the hope is that such an AI would augment human decision-making and foresight, leading to a future where we are better prepared for whatever comes next. 

**Sources:**

1. Good Judgment Inc., *Hybrid Forecasting Competition results (Superforecasters vs. Hybrid Systems)* – summary of a US government tournament showing superforecasters outperforming hybrid human-machine teams ([Supers vs hybrid systems - Good Judgment](https://goodjudgment.com/resources/the-superforecasters-track-record/supers-vs-hybrid-systems/#:~:text=In%20a%20US,later%2C%20the%20results%20were%20clear)).  
2. Good Judgment Inc., *Forecasting Research Institute – ForecastBench Report (2024)* – finding that state-of-the-art language models still underperform human superforecasters by ~19% on forecasting tasks ([The Science Of Superforecasting – Good Judgment](https://goodjudgment.com/about/the-science-of-superforecasting/#:~:text=%282024%29%20arxiv)).  
3. Wikipedia, *Artificial general intelligence* – definition of AGI as AI matching or surpassing human cognitive abilities across wide tasks ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=Artificial%20general%20intelligence%20,the%20definitions%20of%20%20162)).  
4. Wikipedia, *Superforecaster* – explanation of superforecasters as individuals with statistically verifiable, above-average forecasting accuracy ([Superforecaster - Wikipedia](https://en.wikipedia.org/wiki/Superforecaster#:~:text=Forecasters%20whose%20results%20are%20more,accurate%20than%20average)).  
5. Anblicks Blog, *Forecasting Future Events with AI and ML: Capabilities and Limitations* – discussion of current AI prediction uses in finance, healthcare, disasters and limitations like data scarcity, complexity, and human behavior unpredictability ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=Nowadays%C2%A0predictions%20using%20ML%20and%20AI%C2%A0are,that%20need%20to%20be%20overcome)) ([Forecasting Future Events with AI and ML: Capabilities and Limitations](https://www.anblicks.com/blog/forecasting-future-events-with-ai-and-ml/#:~:text=1,events%20depend%20on%20human%20behavior)).  
6. AFP Online, *The Role of AI in Forecasting and Where It Falls Short* – examples of AI in demand forecasting and challenges such as data quality, black swan events, model bias, and need for human judgment ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Volatility%20and%20Uncertainty%3A%20Financial%20markets,the%20pattern%20of%20historical%20data)) ([
	The Role of AI in Forecasting and Where It Falls Short
](https://www.afponline.org/training-resources/resources/articles/Details/the-role-of-ai-in-forecasting-and-where-it-falls-short#:~:text=Human%20Expertise%20and%20Judgment%3A%20While,a%20struggle%20for%20AI%20models)).  
7. Frontiers in AI, Mark Du (2023), *Machine vs. human judgment* – notes that humans are prone to noise and bias, while AI can be more consistent, but current AI lacks human-like intuition and may still introduce errors (implied from discussion) ([Frontiers | Machine vs. human, who makes a better judgment on innovation? Take GPT-4 for example](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1206516/full#:~:text=Results%3A%20Evidence%20indicates%20that%20humans,data%20and%20employ%20logical%20algorithms)) ([Frontiers | Machine vs. human, who makes a better judgment on innovation? Take GPT-4 for example](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1206516/full#:~:text=less%20noise%20than%20humans%20due,and%20promote%20more%20effective%20and)).  
8. Center for AI Safety (CAIS) Blog, *Superhuman Automated Forecasting* – describes a prototype forecasting bot using GPT-4 that matches crowd accuracy, and discusses benefits (speed, scale) and risks (automation bias, self-fulfilling prophecies) of AI forecasting ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=87.0,markets%2C%20and%20they%E2%80%99re%20similarly%20accurate)) ([Superhuman Automated Forecasting | CAIS](https://www.safe.ai/blog/forecasting#:~:text=AIs%20are%20also%20known%20to,a)).  
9. Lawfare, *AI Timelines and National Security: Obstacles to AGI by 2027* – cites surveyed AI researchers’ skepticism about short-term AGI, with median estimates pushing into late 21st century for human-level AI capabilities ([
	AI Timelines and National Security: The Obstacles to AGI by 2027 | Lawfare
](https://www.lawfaremedia.org/article/ai-timelines-and-national-security--the-obstacles-to-agi-by-2027#:~:text=Even%20delaying%20the%20date%20of,could%20skew%20responses%20toward%20rapid)).  
10. SafeCambridge (University of Cambridge) report on *Malicious Use of AI* – warns that advanced AI could be used by rogue actors for nefarious purposes, underlining the need for oversight in powerful AI systems ([Global AI experts sound the alarm in unique report](https://www.cam.ac.uk/stories/malicious-ai-report#:~:text=Global%20AI%20experts%20sound%20the,rogue%20states%2C%20criminals%2C%20and%20terrorists)).  

